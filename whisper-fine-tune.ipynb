{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:01:17.093830Z","iopub.execute_input":"2025-12-13T12:01:17.094035Z","iopub.status.idle":"2025-12-13T12:01:19.077743Z","shell.execute_reply.started":"2025-12-13T12:01:17.094016Z","shell.execute_reply":"2025-12-13T12:01:19.076943Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"gpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') >= 0:\n  print('Not connected to a GPU')\nelse:\n  print(gpu_info)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:01:19.079351Z","iopub.execute_input":"2025-12-13T12:01:19.079957Z","iopub.status.idle":"2025-12-13T12:01:19.139907Z","shell.execute_reply.started":"2025-12-13T12:01:19.079936Z","shell.execute_reply":"2025-12-13T12:01:19.139199Z"}},"outputs":[{"name":"stdout","text":"Sat Dec 13 12:01:19 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 570.172.08             Driver Version: 570.172.08     CUDA Version: 12.8     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0             25W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q \\\n  torchcodec \\\n  datasets==2.18.0 \\\n  transformers==4.40.2 \\\n  accelerate==0.31.0 \\\n  peft==0.11.1 \\\n  evaluate==0.4.1 \\\n  jiwer==3.0.3 \\\n  protobuf==4.25.3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:01:19.140706Z","iopub.execute_input":"2025-12-13T12:01:19.140957Z","iopub.status.idle":"2025-12-13T12:02:43.486623Z","shell.execute_reply.started":"2025-12-13T12:01:19.140932Z","shell.execute_reply":"2025-12-13T12:02:43.485696Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.3 which is incompatible.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2024.2.0 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.2 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import datasets, pyarrow, torch\nprint(\"datasets:\", datasets.__version__)\nprint(\"pyarrow:\", pyarrow.__version__)\nprint(\"torch:\", torch.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:02:43.487823Z","iopub.execute_input":"2025-12-13T12:02:43.488431Z","iopub.status.idle":"2025-12-13T12:02:47.698556Z","shell.execute_reply.started":"2025-12-13T12:02:43.488391Z","shell.execute_reply":"2025-12-13T12:02:47.697898Z"}},"outputs":[{"name":"stdout","text":"datasets: 2.18.0\npyarrow: 19.0.1\ntorch: 2.6.0+cu124\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nlogin(token=hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:02:47.699282Z","iopub.execute_input":"2025-12-13T12:02:47.699680Z","iopub.status.idle":"2025-12-13T12:02:47.984427Z","shell.execute_reply.started":"2025-12-13T12:02:47.699660Z","shell.execute_reply":"2025-12-13T12:02:47.983664Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset, DatasetDict, concatenate_datasets, Audio\n\ncv_hi = load_dataset(\"regisss/common_voice_11_0_hi\")\ncommon_voice = DatasetDict({\n    \"train\": cv_hi[\"train\"],\n    \"validation\": cv_hi[\"validation\"],\n    \"test\": cv_hi[\"test\"]\n})\n\nextra_train = load_dataset(\n    \"damerajee/Hindi-audio-speech\",\n    split=\"train\"\n)\n\nprint(common_voice)\nprint(extra_train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:02:47.985311Z","iopub.execute_input":"2025-12-13T12:02:47.985700Z","iopub.status.idle":"2025-12-13T12:03:35.490096Z","shell.execute_reply.started":"2025-12-13T12:02:47.985676Z","shell.execute_reply":"2025-12-13T12:03:35.489529Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7179938017554e99bc870cc29c047611"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 110M/110M [00:00<00:00, 158MB/s]  \nDownloading data: 100%|██████████| 60.2M/60.2M [00:00<00:00, 179MB/s] \nDownloading data: 100%|██████████| 89.2M/89.2M [00:00<00:00, 203MB/s] \nDownloading data: 100%|██████████| 110M/110M [00:00<00:00, 200MB/s]  \nDownloading data: 100%|██████████| 22.6M/22.6M [00:00<00:00, 114MB/s] \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/4361 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e261c9d8e17848779bcf0d955c2ff820"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2179 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b511ac977f8401d846982d422c7913d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2894 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2410ae1a9a5d44a2a359a816446a81df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating other split:   0%|          | 0/3328 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65b09841413c49afa494e1395346d38d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating invalidated split:   0%|          | 0/680 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eed91d9144204de68c23836d80ba3706"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/479 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fa0cd44d937478a87f6b69c4d058169"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 667M/667M [00:02<00:00, 252MB/s]  \nDownloading data: 100%|██████████| 659M/659M [00:02<00:00, 246MB/s]  \nDownloading data: 100%|██████████| 659M/659M [00:06<00:00, 97.8MB/s] \nDownloading data: 100%|██████████| 634M/634M [00:02<00:00, 255MB/s]  \nDownloading data: 100%|██████████| 683M/683M [00:02<00:00, 250MB/s]  \nDownloading data: 100%|██████████| 655M/655M [00:04<00:00, 152MB/s]  \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2463 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6058958200864a55936eb61e163e2a3f"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n        num_rows: 4361\n    })\n    validation: Dataset({\n        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n        num_rows: 2179\n    })\n    test: Dataset({\n        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment'],\n        num_rows: 2894\n    })\n})\nDataset({\n    features: ['audio_id', 'language', 'audio', 'raw_text', 'gender'],\n    num_rows: 2463\n})\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"cv_train = common_voice[\"train\"].select_columns([\"audio\", \"sentence\"])\ncv_val   = common_voice[\"validation\"].select_columns([\"audio\", \"sentence\"])\ncv_test  = common_voice[\"test\"].select_columns([\"audio\", \"sentence\"])\n\n\ncv_train = cv_train.cast_column(\"audio\", Audio(sampling_rate=16000))\ncv_val   = cv_val.cast_column(\"audio\", Audio(sampling_rate=16000))\ncv_test  = cv_test.cast_column(\"audio\", Audio(sampling_rate=16000))\n\ncv_train_full = concatenate_datasets([cv_train, cv_val])\n\n\nextra_train_clean = (\n    extra_train\n    .select_columns([\"audio\", \"raw_text\"])\n    .rename_column(\"raw_text\", \"sentence\")\n    .cast_column(\"audio\", Audio(sampling_rate=16000))\n)\n\ndataset = DatasetDict({\n    #\"train\": concatenate_datasets([cv_train_full, extra_train_clean]),\n    \"train\": cv_train_full,\n    \"test\": cv_test\n})\n\nprint(dataset)\nprint(dataset[\"train\"][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:03:35.492458Z","iopub.execute_input":"2025-12-13T12:03:35.492956Z","iopub.status.idle":"2025-12-13T12:03:55.886286Z","shell.execute_reply.started":"2025-12-13T12:03:35.492936Z","shell.execute_reply":"2025-12-13T12:03:55.885652Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['audio', 'sentence'],\n        num_rows: 6540\n    })\n    test: Dataset({\n        features: ['audio', 'sentence'],\n        num_rows: 2894\n    })\n})\n{'audio': {'path': 'common_voice_hi_26008353.mp3', 'array': array([ 3.81639165e-17,  2.42861287e-17, -1.73472348e-17, ...,\n       -1.30981789e-07,  2.63096808e-07,  4.77157300e-08]), 'sampling_rate': 16000}, 'sentence': 'हमने उसका जन्मदिन मनाया।'}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from transformers import WhisperFeatureExtractor\n\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:03:55.886920Z","iopub.execute_input":"2025-12-13T12:03:55.887350Z","iopub.status.idle":"2025-12-13T12:03:57.866360Z","shell.execute_reply.started":"2025-12-13T12:03:55.887330Z","shell.execute_reply":"2025-12-13T12:03:57.865794Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32ba7dfa1a6e4eedb9557acecd4c8506"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from transformers import WhisperTokenizer\n\ntokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Hindi\", task=\"transcribe\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:03:57.867050Z","iopub.execute_input":"2025-12-13T12:03:57.867410Z","iopub.status.idle":"2025-12-13T12:03:59.097056Z","shell.execute_reply.started":"2025-12-13T12:03:57.867392Z","shell.execute_reply":"2025-12-13T12:03:59.096499Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d66e5101d11c4b97812fa767700d89a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"942599e9996c451b865e50049f718d45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c2be30054be458c949b67db224dc764"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57e48770c72047aa8c258401649d37d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3be32c2498ba4114b7066168fafd573b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0981e233d184af0b46348d18322c74a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a7812033c5e4b02ba1ee59ac5648719"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from transformers import WhisperProcessor\n\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Hindi\", task=\"transcribe\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:03:59.097888Z","iopub.execute_input":"2025-12-13T12:03:59.098159Z","iopub.status.idle":"2025-12-13T12:03:59.696617Z","shell.execute_reply.started":"2025-12-13T12:03:59.098133Z","shell.execute_reply":"2025-12-13T12:03:59.695806Z"}},"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def prepare_dataset(batch):\n    # load audio data\n    audio = batch[\"audio\"]\n\n    # compute log-Mel input features\n    batch[\"input_features\"] = processor.feature_extractor(\n        audio[\"array\"],\n        sampling_rate=audio[\"sampling_rate\"]\n    ).input_features[0]\n\n    # encode target text to label ids\n    batch[\"labels\"] = processor.tokenizer(\n        batch[\"sentence\"]\n    ).input_ids\n\n    return batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:03:59.697495Z","iopub.execute_input":"2025-12-13T12:03:59.698152Z","iopub.status.idle":"2025-12-13T12:03:59.702525Z","shell.execute_reply.started":"2025-12-13T12:03:59.698115Z","shell.execute_reply":"2025-12-13T12:03:59.701747Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"dataset = dataset.map(\n    prepare_dataset,\n    remove_columns=dataset[\"train\"].column_names,\n    num_proc=4\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:03:59.703261Z","iopub.execute_input":"2025-12-13T12:03:59.703503Z","iopub.status.idle":"2025-12-13T12:07:04.230544Z","shell.execute_reply.started":"2025-12-13T12:03:59.703460Z","shell.execute_reply":"2025-12-13T12:07:04.229071Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/6540 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8ca9794af894de5a55716628b8b6825"}},"metadata":{}},{"name":"stderr","text":"2025-12-13 12:04:02.277276: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-12-13 12:04:02.277291: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-12-13 12:04:02.277283: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-12-13 12:04:02.277269: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765627442.483812     163 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765627442.483815     164 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765627442.483826     166 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765627442.483801     165 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765627442.543106     166 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1765627442.543115     164 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1765627442.543119     163 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1765627442.543129     165 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/2894 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed96c4d3b8be472ebfe7ee790d7bae47"}},"metadata":{}},{"name":"stderr","text":"2025-12-13 12:06:02.969661: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765627563.012427     197 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765627563.024305     197 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-12-13 12:06:03.178230: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-12-13 12:06:03.179043: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-12-13 12:06:03.180970: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765627563.219239     198 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765627563.222037     200 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765627563.222020     199 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765627563.232784     198 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1765627563.234847     200 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1765627563.235724     199 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"split = dataset[\"train\"].train_test_split(\n    test_size=0.2,\n    seed=42\n)\n\ntrain_dataset = split[\"train\"]\neval_dataset  = split[\"test\"]\n\ndataset = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": eval_dataset,\n    \"test\": dataset[\"test\"]\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:07:04.231594Z","iopub.execute_input":"2025-12-13T12:07:04.231876Z","iopub.status.idle":"2025-12-13T12:07:04.253861Z","shell.execute_reply.started":"2025-12-13T12:07:04.231841Z","shell.execute_reply":"2025-12-13T12:07:04.252813Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from transformers import WhisperForConditionalGeneration\n\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:07:04.254596Z","iopub.execute_input":"2025-12-13T12:07:04.254859Z","iopub.status.idle":"2025-12-13T12:07:12.163234Z","shell.execute_reply.started":"2025-12-13T12:07:04.254837Z","shell.execute_reply":"2025-12-13T12:07:12.162631Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6ce5093c275458eb017217b86f91570"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a6b023635a2404f87e8181dc5112dfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cb667c1fb0e49498611b27850123aad"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"model.generation_config.language = \"hindi\"\nmodel.generation_config.task = \"transcribe\"\n\nmodel.generation_config.forced_decoder_ids = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:07:12.164239Z","iopub.execute_input":"2025-12-13T12:07:12.164678Z","iopub.status.idle":"2025-12-13T12:07:12.168575Z","shell.execute_reply.started":"2025-12-13T12:07:12.164660Z","shell.execute_reply":"2025-12-13T12:07:12.167740Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import torch\n\nfrom dataclasses import dataclass\nfrom typing import Any, Dict, List, Union\n\n@dataclass\nclass DataCollatorSpeechSeq2SeqWithPadding:\n    processor: Any\n    decoder_start_token_id: int\n\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n        # split inputs and labels since they have to be of different lengths and need different padding methods\n        # first treat the audio inputs by simply returning torch tensors\n        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n\n        # get the tokenized label sequences\n        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n        # pad the labels to max length\n        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n\n        # replace padding with -100 to ignore loss correctly\n        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n\n        # if bos token is appended in previous tokenization step,\n        # cut bos token here as it's append later anyways\n        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n            labels = labels[:, 1:]\n\n        batch[\"labels\"] = labels\n\n        return batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:07:12.169349Z","iopub.execute_input":"2025-12-13T12:07:12.169778Z","iopub.status.idle":"2025-12-13T12:07:12.210035Z","shell.execute_reply.started":"2025-12-13T12:07:12.169754Z","shell.execute_reply":"2025-12-13T12:07:12.209507Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n    processor=processor,\n    decoder_start_token_id=model.config.decoder_start_token_id\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:07:12.210756Z","iopub.execute_input":"2025-12-13T12:07:12.210980Z","iopub.status.idle":"2025-12-13T12:07:12.223168Z","shell.execute_reply.started":"2025-12-13T12:07:12.210958Z","shell.execute_reply":"2025-12-13T12:07:12.222461Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import evaluate\n\nmetric = evaluate.load(\"wer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:07:12.223924Z","iopub.execute_input":"2025-12-13T12:07:12.224197Z","iopub.status.idle":"2025-12-13T12:07:16.870430Z","shell.execute_reply.started":"2025-12-13T12:07:12.224175Z","shell.execute_reply":"2025-12-13T12:07:16.869803Z"}},"outputs":[{"name":"stderr","text":"2025-12-13 12:07:12.514124: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765627632.535393      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765627632.541733      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"673b1485d2ba43d6ac389436c9ac665a"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"def compute_metrics(pred):\n    pred_ids = pred.predictions\n    if isinstance(pred_ids, tuple):\n        pred_ids = pred_ids[0]\n\n    label_ids = pred.label_ids\n\n    label_ids[label_ids == -100] = tokenizer.pad_token_id\n\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n\n    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"wer\": wer}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:07:16.871286Z","iopub.execute_input":"2025-12-13T12:07:16.872117Z","iopub.status.idle":"2025-12-13T12:07:16.876768Z","shell.execute_reply.started":"2025-12-13T12:07:16.872092Z","shell.execute_reply":"2025-12-13T12:07:16.875910Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from transformers import Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"whisper-small-hi-asr\",\n    hub_model_id=\"iamgarvit/whisper-small-hi-asr\",\n    per_device_train_batch_size=16,\n    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n    learning_rate=1e-5,\n    warmup_steps=500,\n    max_steps=4000,\n    gradient_checkpointing=True,\n    fp16=True,\n    evaluation_strategy=\"steps\",\n    per_device_eval_batch_size=8,\n    predict_with_generate=True,\n    generation_max_length=225,\n    save_steps=1000,\n    eval_steps=1000,\n    logging_steps=25,\n    report_to=[\"tensorboard\"],\n    load_best_model_at_end=True,\n    metric_for_best_model=\"wer\",\n    greater_is_better=False,\n    push_to_hub=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:07:16.877589Z","iopub.execute_input":"2025-12-13T12:07:16.877939Z","iopub.status.idle":"2025-12-13T12:07:17.008305Z","shell.execute_reply.started":"2025-12-13T12:07:16.877915Z","shell.execute_reply":"2025-12-13T12:07:17.007511Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"validation\"],\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    tokenizer=processor.feature_extractor,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:07:17.009145Z","iopub.execute_input":"2025-12-13T12:07:17.009426Z","iopub.status.idle":"2025-12-13T12:07:18.304005Z","shell.execute_reply.started":"2025-12-13T12:07:17.009400Z","shell.execute_reply":"2025-12-13T12:07:18.303427Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:477: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"processor.save_pretrained(training_args.output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:07:18.306947Z","iopub.execute_input":"2025-12-13T12:07:18.307163Z","iopub.status.idle":"2025-12-13T12:07:18.830810Z","shell.execute_reply.started":"2025-12-13T12:07:18.307146Z","shell.execute_reply":"2025-12-13T12:07:18.830032Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"[]"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T12:07:18.831643Z","iopub.execute_input":"2025-12-13T12:07:18.831901Z","iopub.status.idle":"2025-12-13T20:14:01.048281Z","shell.execute_reply.started":"2025-12-13T12:07:18.831878Z","shell.execute_reply":"2025-12-13T20:14:01.047522Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4000/4000 8:06:17, Epoch 12/13]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Wer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>0.061100</td>\n      <td>0.204290</td>\n      <td>26.448210</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.006100</td>\n      <td>0.249188</td>\n      <td>25.263704</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.000600</td>\n      <td>0.286658</td>\n      <td>24.961093</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.000200</td>\n      <td>0.300594</td>\n      <td>24.900571</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\nThere were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4000, training_loss=0.07107430141803343, metrics={'train_runtime': 29186.7048, 'train_samples_per_second': 2.193, 'train_steps_per_second': 0.137, 'total_flos': 1.846946562048e+19, 'train_loss': 0.07107430141803343, 'epoch': 12.232415902140673})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"kwargs = {\n    \"dataset_tags\": [\"common_voice\",\"hindi\",\"speech\"],\n    \"dataset\": \"Common Voice Hindi + Hindi Audio Speech\",\n    \"dataset_args\": \"language: hi; splits: train/validation\",\n    \"language\": \"hi\",\n    \"model_name\": \"Whisper Small Hindi ASR\",\n    \"finetuned_from\": \"openai/whisper-small\",\n    \"tasks\": \"automatic-speech-recognition\",\n    \"tags\": [\"whisper\",\"asr\",\"hindi\",\"speech-recognition\",\"kaggle\"],\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T20:14:01.049099Z","iopub.execute_input":"2025-12-13T20:14:01.049387Z","iopub.status.idle":"2025-12-13T20:14:01.053584Z","shell.execute_reply.started":"2025-12-13T20:14:01.049362Z","shell.execute_reply":"2025-12-13T20:14:01.052854Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"trainer.push_to_hub(**kwargs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T20:14:01.054351Z","iopub.execute_input":"2025-12-13T20:14:01.054559Z","iopub.status.idle":"2025-12-13T20:14:14.482387Z","shell.execute_reply.started":"2025-12-13T20:14:01.054544Z","shell.execute_reply":"2025-12-13T20:14:14.481776Z"}},"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9791014601ec4bed9b66a1886d1d33ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d5c70be55a44f31a8e086b225891250"}},"metadata":{}},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/iamgarvit/whisper-small-hi-asr/commit/d189f571e2f13112dd1c052807c51ed1be983f00', commit_message='End of training', commit_description='', oid='d189f571e2f13112dd1c052807c51ed1be983f00', pr_url=None, repo_url=RepoUrl('https://huggingface.co/iamgarvit/whisper-small-hi-asr', endpoint='https://huggingface.co', repo_type='model', repo_id='iamgarvit/whisper-small-hi-asr'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"from transformers import pipeline\nimport gradio as gr\nimport torch\n\npipe = pipeline(\n    task=\"automatic-speech-recognition\",\n    model=\"iamgarvit/whisper-small-hi-asr\",\n    device=0 if torch.cuda.is_available() else -1,\n)\n\ndef transcribe(audio_path):\n    if audio_path is None:\n        return \"\"\n    result = pipe(audio_path)\n    return result[\"text\"]\n\niface = gr.Interface(\n    fn=transcribe,\n    inputs=gr.Audio(\n        type=\"filepath\",\n        label=\"Speak using microphone or upload an audio file\",\n    ),\n    outputs=gr.Textbox(label=\"Transcription\"),\n    title=\"Whisper Small Hindi ASR\",\n    description=\"Hindi speech recognition using a fine-tuned Whisper-small model.\",\n)\n\niface.launch()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T20:16:54.578673Z","iopub.execute_input":"2025-12-13T20:16:54.579327Z","iopub.status.idle":"2025-12-13T20:16:56.871969Z","shell.execute_reply.started":"2025-12-13T20:16:54.579303Z","shell.execute_reply":"2025-12-13T20:16:56.871375Z"}},"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7860\nIt looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://f08efc67b6cf5a17dd.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://f08efc67b6cf5a17dd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":27}]}